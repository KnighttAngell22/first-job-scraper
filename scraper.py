import discord
import asyncio
import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
import re
import json

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LiveOpportunityScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # Track what we've found to avoid duplicates
        self.found_opportunities = []
    
    def clean_text(self, text):
        """Clean and normalize text"""
        if not text:
            return ""
        return ' '.join(text.strip().split())
    
    def extract_funding_amount(self, text):
        """Extract funding amounts from text"""
        if not text:
            return "Check website"
        
        # Look for common funding patterns
        patterns = [
            r'‚Çπ[\d,.]+ (?:lakh|crore)',
            r'\$[\d,.]+ ?(?:k|million)',
            r'prize.*?‚Çπ[\d,]+',
            r'funding.*?‚Çπ[\d,]+',
            r'grant.*?‚Çπ[\d,]+'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        
        return "Check website"
    
    def scrape_unstop_live(self):
        """REAL scraping from Unstop hackathons"""
        opportunities = []
        
        try:
            url = "https://unstop.com/hackathons"
            logger.info(f"üîç Scraping Unstop: {url}")
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for hackathon cards - try multiple selectors
            selectors = [
                'div[class*="card"]',
                'div[class*="challenge"]', 
                'div[class*="opportunity"]',
                'article',
                'div[class*="event"]'
            ]
            
            hackathon_elements = []
            for selector in selectors:
                elements = soup.select(selector)
                hackathon_elements.extend(elements)
                if len(elements) > 0:
                    logger.info(f"Found {len(elements)} elements with selector: {selector}")
            
            logger.info(f"Total elements found: {len(hackathon_elements)}")
            
            processed = 0
            for element in hackathon_elements[:20]:  # Limit processing
                try:
                    # Find title/name
                    title_selectors = ['h1', 'h2', 'h3', 'h4', 'h5', 'a[href*="hackathon"]', 'a[href*="challenge"]']
                    title = None
                    link = None
                    
                    for selector in title_selectors:
                        title_elem = element.select_one(selector)
                        if title_elem:
                            title = self.clean_text(title_elem.get_text())
                            if len(title) > 10 and not any(skip in title.lower() for skip in ['login', 'register', 'menu', 'footer']):
                                if title_elem.name == 'a':
                                    link = title_elem.get('href', '')
                                break
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # Find link if not found yet
                    if not link:
                        link_elem = element.select_one('a')
                        if link_elem:
                            link = link_elem.get('href', '')
                    
                    # Make link absolute
                    if link and not link.startswith('http'):
                        link = f"https://unstop.com{link}"
                    
                    # Extract deadline/date info
                    deadline = "Check website"
                    date_patterns = [
                        r'\d{1,2}[-/]\d{1,2}[-/]\d{4}',
                        r'\d{1,2} \w+ \d{4}',
                        r'deadline.*?\d+',
                        r'ends.*?\d+',
                        r'due.*?\d+'
                    ]
                    
                    element_text = element.get_text()
                    for pattern in date_patterns:
                        match = re.search(pattern, element_text, re.IGNORECASE)
                        if match:
                            deadline = match.group(0)
                            break
                    
                    # Extract prize/funding
                    funding = self.extract_funding_amount(element_text)
                    
                    opportunity = {
                        "name": title[:80],
                        "provider": "Unstop Platform",
                        "sector": "Technical",
                        "funding": funding,
                        "deadline": deadline,
                        "link": link or url,
                        "status": "üî¥ LIVE from Unstop",
                        "scraped_at": datetime.now().isoformat()
                    }
                    
                    # Avoid duplicates
                    if not any(existing['name'] == opportunity['name'] for existing in opportunities):
                        opportunities.append(opportunity)
                        processed += 1
                        logger.info(f"‚úÖ Scraped: {title[:50]}...")
                        
                        if processed >= 5:  # Limit to 5 per source
                            break
                
                except Exception as e:
                    logger.debug(f"Error processing element: {e}")
                    continue
            
            logger.info(f"‚úÖ Unstop: Found {len(opportunities)} opportunities")
            
        except Exception as e:
            logger.error(f"‚ùå Unstop scraping failed: {e}")
            # Add fallback opportunity
            opportunities.append({
                "name": "Unstop Hackathons (Manual Check Required)",
                "provider": "Unstop",
                "sector": "Technical",
                "funding": "Various prizes available",
                "deadline": "Multiple deadlines",
                "link": "https://unstop.com/hackathons",
                "status": "‚ö†Ô∏è Scraping failed - visit manually",
                "scraped_at": datetime.now().isoformat()
            })
        
        return opportunities
    
    def scrape_devfolio_live(self):
        """REAL scraping from Devfolio"""
        opportunities = []
        
        try:
            url = "https://devfolio.co/hackathons"
            logger.info(f"üîç Scraping Devfolio: {url}")
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Look for hackathon cards
            hackathon_links = soup.find_all('a', href=True)
            
            processed = 0
            for link in hackathon_links:
                href = link.get('href', '')
                text = self.clean_text(link.get_text())
                
                # Filter for hackathon-related links
                if any(keyword in href.lower() for keyword in ['hackathon', 'event']) and len(text) > 10:
                    
                    if not href.startswith('http'):
                        href = f"https://devfolio.co{href}"
                    
                    opportunity = {
                        "name": text[:80],
                        "provider": "Devfolio",
                        "sector": "Technical", 
                        "funding": "Check event page",
                        "deadline": "Check website",
                        "link": href,
                        "status": "üî¥ LIVE from Devfolio",
                        "scraped_at": datetime.now().isoformat()
                    }
                    
                    # Avoid duplicates
                    if not any(existing['name'] == opportunity['name'] for existing in opportunities):
                        opportunities.append(opportunity)
                        processed += 1
                        logger.info(f"‚úÖ Scraped: {text[:50]}...")
                        
                        if processed >= 3:  # Limit to 3 per source
                            break
            
            logger.info(f"‚úÖ Devfolio: Found {len(opportunities)} opportunities")
            
        except Exception as e:
            logger.error(f"‚ùå Devfolio scraping failed: {e}")
            opportunities.append({
                "name": "Devfolio Hackathons (Manual Check Required)",
                "provider": "Devfolio",
                "sector": "Technical",
                "funding": "Various prizes",
                "deadline": "Multiple events",
                "link": "https://devfolio.co/hackathons",
                "status": "‚ö†Ô∏è Scraping failed - visit manually",
                "scraped_at": datetime.now().isoformat()
            })
        
        return opportunities
    
    def scrape_startup_india_live(self):
        """REAL scraping from Startup India"""
        opportunities = []
        
        try:
            # Try multiple Startup India URLs
            urls = [
                "https://www.startupindia.gov.in/content/sih/en/government_schemes.html",
                "https://www.startupindia.gov.in/"
            ]
            
            for url in urls:
                try:
                    logger.info(f"üîç Scraping Startup India: {url}")
                    response = self.session.get(url, timeout=30)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Look for scheme mentions
                    scheme_keywords = ['scheme', 'fund', 'grant', 'support', 'program']
                    
                    for keyword in scheme_keywords:
                        elements = soup.find_all(text=re.compile(keyword, re.IGNORECASE))
                        
                        for element in elements[:5]:
                            parent = element.parent
                            if parent:
                                text = self.clean_text(parent.get_text())
                                
                                if 20 < len(text) < 200 and keyword.lower() in text.lower():
                                    opportunity = {
                                        "name": f"Startup India: {text[:60]}...",
                                        "provider": "Government of India",
                                        "sector": "Generalist",
                                        "funding": "Government support",
                                        "deadline": "Check website",
                                        "link": url,
                                        "status": "üî¥ LIVE from Startup India",
                                        "scraped_at": datetime.now().isoformat()
                                    }
                                    
                                    if not any(existing['name'] == opportunity['name'] for existing in opportunities):
                                        opportunities.append(opportunity)
                                        logger.info(f"‚úÖ Scraped: {text[:40]}...")
                                        
                                        if len(opportunities) >= 2:
                                            break
                        
                        if len(opportunities) >= 2:
                            break
                    
                    if len(opportunities) > 0:
                        break
                        
                except Exception as e:
                    logger.debug(f"Failed to scrape {url}: {e}")
                    continue
            
            logger.info(f"‚úÖ Startup India: Found {len(opportunities)} opportunities")
            
        except Exception as e:
            logger.error(f"‚ùå Startup India scraping failed: {e}")
        
        # Always add at least one known opportunity
        if len(opportunities) == 0:
            opportunities.append({
                "name": "Startup India Seed Fund Scheme",
                "provider": "DPIIT, Government of India",
                "sector": "Generalist",
                "funding": "Up to ‚Çπ20 Lakh",
                "deadline": "Rolling applications",
                "link": "https://www.startupindia.gov.in/content/sih/en/government-schemes/startup-india-seed-fund-scheme.html",
                "status": "üèõÔ∏è Government Scheme",
                "scraped_at": datetime.now().isoformat()
            })
        
        return opportunities
    
    def scrape_all_sources(self):
        """Scrape all sources and combine results"""
        all_opportunities = []
        
        logger.info("üöÄ Starting LIVE web scraping from multiple sources...")
        
        # Scrape each source
        unstop_opps = self.scrape_unstop_live()
        devfolio_opps = self.scrape_devfolio_live() 
        startup_opps = self.scrape_startup_india_live()
        
        all_opportunities.extend(unstop_opps)
        all_opportunities.extend(devfolio_opps)
        all_opportunities.extend(startup_opps)
        
        logger.info(f"üéØ Total opportunities scraped: {len(all_opportunities)}")
        
        return all_opportunities

# Discord Bot
async def send_to_discord(opportunities):
    token = os.getenv('DISCORD_BOT_TOKEN')
    channel_id = os.getenv('DISCORD_CHANNEL_ID')
    
    if not token or not channel_id:
        logger.error("‚ùå Missing Discord credentials")
        return
    
    try:
        client = discord.Client(intents=discord.Intents.default())
        
        @client.event
        async def on_ready():
            channel = client.get_channel(int(channel_id))
            if channel:
                # Header with scraping timestamp
                embed = discord.Embed(
                    title="üî¥ LIVE Scraped Opportunities",
                    description=f"**{len(opportunities)} fresh opportunities** found by scraping websites in real-time!",
                    color=0xff4444,
                    timestamp=datetime.now()
                )
                embed.add_field(name="ü§ñ Scraping Status", value="Automated web scraping active", inline=False)
                embed.set_footer(text="Real-time data from live websites")
                await channel.send(embed=embed)
                
                # Send each opportunity
                for i, opp in enumerate(opportunities, 1):
                    opp_embed = discord.Embed(
                        title=f"{i}. {opp['name']}",
                        color=0x4287f5,
                        url=opp["link"]
                    )
                    opp_embed.add_field(name="üè¢ Provider", value=opp["provider"], inline=True)
                    opp_embed.add_field(name="üéØ Sector", value=opp["sector"], inline=True)
                    opp_embed.add_field(name="üí∞ Funding", value=opp["funding"], inline=True)
                    opp_embed.add_field(name="‚è∞ Deadline", value=opp["deadline"], inline=True)
                    opp_embed.add_field(name="üïê Scraped", value=opp["scraped_at"].split('T')[0], inline=True)
                    opp_embed.add_field(name=opp['status'], value=f"[Visit Website]({opp['link']})", inline=False)
                    
                    await channel.send(embed=opp_embed)
                
                logger.info("‚úÖ All opportunities sent to Discord!")
            
            await client.close()
        
        await client.start(token)
        
    except Exception as e:
        logger.error(f"‚ùå Discord error: {e}")

# Main function
async def main():
    logger.info("üöÄ Starting REAL LIVE WEB SCRAPING")
    
    scraper = LiveOpportunityScraper()
    opportunities = scraper.scrape_all_sources()
    
    if opportunities:
        await send_to_discord(opportunities)
        logger.info("‚úÖ Live scraping completed successfully!")
    else:
        logger.warning("‚ö†Ô∏è No opportunities found")

if __name__ == "__main__":
    asyncio.run(main())
